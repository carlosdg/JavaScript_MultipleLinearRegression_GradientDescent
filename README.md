# Multiple linear regression using gradient descent
<h2> Brief description: </h2>
  <p> 
    Implemented a general algorithm to solve linear regression problems with more than
    one independent variable using gradient descent. Gradient descent is used to get an 
    aproximation of the minimum in the cost function (the least square error) so we can
    get the optimal parameters for the linear function.
 </p>

<h2> Author: </h2>
<p>	Carlos Domínguez García </p>

<h2> Tested on: </h2>
<ul>
  <li>	Google Chrome   - 59.0.3071.86  </li>
	<li>  Mozilla Firefox - 54.0          </li>
</ul>

<h2> How it works: </h2>
  <p>
    Though the algorithms are for any number of independent variables, for the demo we are
    using only one variable, this way is easier to visualize the problem. The program has a
    canvas where you click to put points, then you can click on the button "Display line"
    to train the intelligent agent with the points in canvas as training data. And 
    the linear function is drawn in the canvas.
  </p>
